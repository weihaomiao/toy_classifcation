Toy Classification using Machine Learning â€” MITx 6.86x Coursework

This repository demonstrates implementations of the Perceptron, Average Perceptron, and Pegasos algorithms for binary classification. Each algorithm is coded from scratch, including parameter updates and the search for optimal 
ğœƒ
Î¸ and 
ğœƒ
0
Î¸
0
	â€‹

.

1. Algorithms
1.1 Perceptron

Standard binary classifier.

Updates only on misclassified points:

If 
ğ‘¦
ğ‘–
(
ğœƒ
â‹…
ğ‘¥
ğ‘–
+
ğœƒ
0
)
â‰¤
0
:
ğœƒ
â†
ğœƒ
+
ğ‘¦
ğ‘–
ğ‘¥
ğ‘–
,
ğœƒ
0
â†
ğœƒ
0
+
ğ‘¦
ğ‘–
If y
i
	â€‹

(Î¸â‹…x
i
	â€‹

+Î¸
0
	â€‹

)â‰¤0:Î¸â†Î¸+y
i
	â€‹

x
i
	â€‹

,Î¸
0
	â€‹

â†Î¸
0
	â€‹

+y
i
	â€‹


Visuals:

Decision boundary with toy data:


Updates occur only on mistakes:


1.2 Average Perceptron

Identical update rule as Perceptron.

Key difference: averages all 
ğœƒ
Î¸ and 
ğœƒ
0
Î¸
0
	â€‹

 values across training steps, including unchanged ones.

ğœƒ
Ë‰
=
1
ğ‘
âˆ‘
ğ‘¡
=
1
ğ‘
ğœƒ
(
ğ‘¡
)
,
ğœƒ
Ë‰
0
=
1
ğ‘
âˆ‘
ğ‘¡
=
1
ğ‘
ğœƒ
0
(
ğ‘¡
)
Î¸
Ë‰
=
N
1
	â€‹

t=1
âˆ‘
N
	â€‹

Î¸
(t)
,
Î¸
Ë‰
0
	â€‹

=
N
1
	â€‹

t=1
âˆ‘
N
	â€‹

Î¸
0
(t)
	â€‹


Visuals:

Updates recorded for every training sample:


1.3 Pegasos (Stochastic Gradient Descent for SVM)

Uses hinge loss and regularization for binary classification.

Update rule:

ğœƒ
â†
(
1
âˆ’
ğœ‚
ğœ†
)
ğœƒ
+
ğœ‚
ğ‘¦
ğ‘–
ğ‘¥
ğ‘–
if margin violated
,
ğœƒ
0
â†
ğœƒ
0
+
ğœ‚
ğ‘¦
ğ‘–
Î¸â†(1âˆ’Î·Î»)Î¸+Î·y
i
	â€‹

x
i
	â€‹

if margin violated,Î¸
0
	â€‹

â†Î¸
0
	â€‹

+Î·y
i
	â€‹


Objective function:

min
â¡
ğœƒ
,
ğœƒ
0
ğœ†
2
âˆ¥
ğœƒ
âˆ¥
2
+
1
ğ‘›
âˆ‘
ğ‘–
=
1
ğ‘›
max
â¡
(
0
,
1
âˆ’
ğ‘¦
ğ‘–
(
ğœƒ
â‹…
ğ‘¥
ğ‘–
+
ğœƒ
0
)
)
Î¸,Î¸
0
	â€‹

min
	â€‹

2
Î»
	â€‹

âˆ¥Î¸âˆ¥
2
+
n
1
	â€‹

i=1
âˆ‘
n
	â€‹

max(0,1âˆ’y
i
	â€‹

(Î¸â‹…x
i
	â€‹

+Î¸
0
	â€‹

))

Regularization term (
ğœ†
Î») controls the margin and generalization.

Visuals:

Decision boundary with Pegasos:


Hinge loss illustration:


Effect of regularization:


Update rule visualization:


Tip: Use distinct line styles, colors, or markers for each algorithm in plots to make comparisons clear.

2. Workflow

Data Exploration â€“ Visualize toy datasets and understand feature distribution.

Parameter Optimization â€“ Find best 
ğœƒ
Î¸ and 
ğœƒ
0
Î¸
0
	â€‹

 for each algorithm.

Visualization â€“ Plot decision boundaries and updates; clearly distinguish lines for Perceptron, Average Perceptron, and Pegasos.

Evaluation â€“ Compare classification accuracy and behavior across algorithms.


